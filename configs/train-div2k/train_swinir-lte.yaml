train_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Training_HR_Small                         
      repeat: 1     # reduced from 20 to make training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      augment: true
      sample_q: 2304
  batch_size: 4      # Reduced from 32 batch due to GPU memory constraints

val_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Validation_HR             
      # first_k: 10 # Use all 60
      repeat: 1     # Reduced from 160 to make the training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      sample_q: 2304
  batch_size: 4     # Reduce from 32 due to GPU memory constraints

data_norm:
  inp: {sub: [0.5], div: [0.5]}
  gt: {sub: [0.5], div: [0.5]}

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        no_upsampling: true
    imnet_spec:
      name: mlp
      args:
        out_dim: 3
        hidden_list: [256, 256, 256]
    hidden_dim: 256

optimizer:
  name: adam
  args:
    lr: 2.e-4     # Used to be 2e-4.
epoch_max: 500    # Used to be 1000, made it less for shorter training
multi_step_lr:
  milestones: [500]  # Adjusted the scheduler appropriately with the max epochs
  gamma: 0.5

epoch_val: 10     # Validate less frequently to save time
epoch_save: 10    # Changed to save more frequently

resume: ./save/_train_swinir-lte/epoch-last.pth