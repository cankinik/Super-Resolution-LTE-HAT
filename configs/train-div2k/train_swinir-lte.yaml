train_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Training_HR
      repeat: 1     # reduced from 20 to make training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      augment: true
      sample_q: 2304
  batch_size: 4      # Reduced from 32 batch due to GPU RAM

val_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Validation_HR
      first_k: 10
      repeat: 5     # Reduced from 160 to make the training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      sample_q: 2304
  batch_size: 4     # Reduce from 32 due to GPU RAM

data_norm:
  inp: {sub: [0.5], div: [0.5]}
  gt: {sub: [0.5], div: [0.5]}

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        no_upsampling: true
    imnet_spec:
      name: mlp
      args:
        out_dim: 3
        hidden_list: [256, 256, 256]
    hidden_dim: 256

optimizer:
  name: adam
  args:
    lr: 2.e-4
epoch_max: 1000
multi_step_lr:
  milestones: [500, 800, 900, 950]
  gamma: 0.5

epoch_val: 1
epoch_save: 100

resume: ./save/_train_swinir-lte/epoch-last.pth