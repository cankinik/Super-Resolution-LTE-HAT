train_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Training_HR
      repeat: 1     # reduced from 20 to make training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      augment: true
      sample_q: 2304
  batch_size: 5      # Reduced from 32 batch due to GPU RAM

val_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./load/DIV2K/Validation_HR
      # first_k: 10 # Use all 60
      repeat: 1     # Reduced from 160 to make the training faster
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      sample_q: 2304
  batch_size: 5     # Reduce from 32 due to GPU memory constraints

data_norm:
  inp: {sub: [0.5], div: [0.5]}
  gt: {sub: [0.5], div: [0.5]}

model:
  name: lte
  args:
    encoder_spec:
      name: swinir
      args:
        no_upsampling: true
    imnet_spec:
      name: mlp
      args:
        out_dim: 3
        hidden_list: [256, 256, 256]
    hidden_dim: 256

optimizer:
  name: adam
  args:
    lr: 1.e-3     # Used to be 2e-4.
epoch_max: 500    # Used to be 1000, made it less for shorter training
multi_step_lr:
  milestones: [100, 200, 300, 400]  # Adjusted the scheduler appropriately with the max epochs
  gamma: 0.5

epoch_val: 1
epoch_save: 20    # Changed to save more frequently

resume: ./save/_train_swinir-lte/epoch-last.pth