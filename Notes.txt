Demo is not generating nice results, but PSNR from test makes sense.
Using x2
Testing: python test.py --config configs/test/test-div2k-2.yaml --model save/_train_swinir-lte/epoch-best.pth --window 8 --gpu 0 
Training: python train.py --config configs/train-div2k/train_swinir-lte.yaml --gpu 0
If you want to test with the pretrained weights, change the number of heads and their depths, and use the appropriate pth.
We should think about selecting a different learning rate (ask the prof if we should try larger or lower), and perhaps reducing epoch but increasing network size.


Things we changed:
lib to Lib in one directory of anaconda
Number of workers of the data loader from 8 to 1 so that it would fit in the regular ram
Batch size of the inputs at training from 32 to 4 so that it fits into our GPU RAM
Number of repeats of the training and validation datasets to 1 so that training would be faster
Reduced the number of heads and their depths for a smaller model
Using DIV2K: 300 for training, 60 for validation. Number of images shouldn't cause overfitting since it is a generative model. 20-80% is a common ratio for validation.
Using SET14 for testing (PSNR calculation)


Tried Overfitting over a small dataset, but it didn't work (loss osciallates from the start regardless of LR). 
	-> Perhaps that is because we have the regularization terms?

2e-4 starts off good, but then stops getting better. 
	-> Not sure if that means it was too high and converged to a bad minima, or if it was too low and only got through the first steep part.

Tried using the downsampled images, but it didn't seem to speed it up, and still couldn't use higher batch size?

Tried using 900 images for training, but just [6, 6] as opposed to [6, 6, 6, 6] and with lr=2e-4
	-> Seems comparable but more stable. Also, it may have benefited from an LR drop at 100. Try that maybe?

Tried using 900 images but this time with [2, 2, 2, 2, 2, 2] and with lr=2e-4
	-> This time, I was able to increase the batch size to 16!

Look into the input image size: Are we actually giving it 3x48x48 images? If so, we should either bump it up, or use another dataset.
