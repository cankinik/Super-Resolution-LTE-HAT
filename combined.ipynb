{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file combines everything in the project into a single view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that CUDA is available, and import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.12.1\n",
      "Torchvision Version:  0.13.1\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import math\n",
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import datasets\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from torch.optim import SGD, Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep track of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "\n",
    "def register(name):\n",
    "    def decorator(cls):\n",
    "        models[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def make(model_spec, args=None, load_sd=False):\n",
    "    if args is not None:\n",
    "        model_args = copy.deepcopy(model_spec['args'])\n",
    "        model_args.update(args)\n",
    "    else:\n",
    "        model_args = model_spec['args']\n",
    "    model = models[model_spec['name']](**model_args)\n",
    "    if load_sd:\n",
    "        model.load_state_dict(model_spec['sd'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        H, W = x_size\n",
    "        B, L, C = x.shape\n",
    "        # assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        if self.input_resolution == x_size:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, x_size)\n",
    "            else:\n",
    "                x = blk(x, x_size)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 img_size=224, patch_size=4, resi_connection='1conv'):\n",
    "        super(RSTB, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.residual_group = BasicLayer(dim=dim,\n",
    "                                         input_resolution=input_resolution,\n",
    "                                         depth=depth,\n",
    "                                         num_heads=num_heads,\n",
    "                                         window_size=window_size,\n",
    "                                         mlp_ratio=mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                         drop=drop, attn_drop=attn_drop,\n",
    "                                         drop_path=drop_path,\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         downsample=downsample,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n",
    "                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.residual_group.flops()\n",
    "        H, W = self.input_resolution\n",
    "        flops += H * W * self.dim * self.dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        flops += self.patch_unembed.flops()\n",
    "\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.img_size\n",
    "        if self.norm is not None:\n",
    "            flops += H * W * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "class UpsampleOneStep(nn.Sequential):\n",
    "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
    "       Used in lightweight SR to save parameters.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
    "        self.num_feat = num_feat\n",
    "        self.input_resolution = input_resolution\n",
    "        m = []\n",
    "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "        super(UpsampleOneStep, self).__init__(*m)\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.num_feat * 3 * 9\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    r\"\"\" SwinIR\n",
    "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=48, patch_size=1, in_chans=3,\n",
    "                #  embed_dim=180, depths=[6, 6, 6, 6, 6, 6], num_heads=[6, 6, 6, 6, 6, 6],              # Trying the full model to see how it fares\n",
    "                 embed_dim=180, depths=[6, 6, 6], num_heads=[6, 6, 6],            # Making the model smaller                \n",
    "                 window_size=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, upscale=2, img_range=1., upsampler='none', resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(SwinIR, self).__init__()\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "        self.window_size = window_size\n",
    "        self.out_dim = num_feat\n",
    "        #####################################################################################################\n",
    "        ################################### 1, shallow feature extraction ###################################\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################### 2, deep feature extraction ######################################\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build Residual Swin Transformer blocks (RSTB)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RSTB(dim=embed_dim,\n",
    "                         input_resolution=(patches_resolution[0],\n",
    "                                           patches_resolution[1]),\n",
    "                         depth=depths[i_layer],\n",
    "                         num_heads=num_heads[i_layer],\n",
    "                         window_size=window_size,\n",
    "                         mlp_ratio=self.mlp_ratio,\n",
    "                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                         norm_layer=norm_layer,\n",
    "                         downsample=None,\n",
    "                         use_checkpoint=use_checkpoint,\n",
    "                         img_size=img_size,\n",
    "                         patch_size=patch_size,\n",
    "                         resi_connection=resi_connection\n",
    "\n",
    "                         )\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################ 3, high quality image reconstruction ################################\n",
    "        if self.upsampler == 'none':\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))        \n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.upsample = Upsample(upscale, num_feat)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR (to save parameters)\n",
    "            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n",
    "                                            (patches_resolution[0], patches_resolution[1]))\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR (less artifacts)\n",
    "            assert self.upscale == 4, 'only support x4 now.'\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x_size)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.patch_unembed(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.check_image_size(x)\n",
    "        \n",
    "#         self.mean = self.mean.type_as(x)\n",
    "#         x = (x - self.mean) * self.img_range\n",
    "        \n",
    "        if self.upsampler == 'none':\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.conv_last(self.upsample(x))\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.upsample(x)\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            x_first = self.conv_first(x)\n",
    "            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n",
    "            x = x + self.conv_last(res)\n",
    "\n",
    "#         x = x / self.img_range + self.mean\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.patches_resolution\n",
    "        flops += H * W * 3 * self.embed_dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += H * W * 3 * self.embed_dim * self.embed_dim\n",
    "        flops += self.upsample.flops()\n",
    "        return flops\n",
    "\n",
    "@register('swinir')\n",
    "def make_swinir(no_upsampling=True):\n",
    "    return SwinIR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('lte')\n",
    "class LTE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_spec, imnet_spec=None, hidden_dim=256):\n",
    "        super().__init__()        \n",
    "        self.encoder = make(encoder_spec)\n",
    "        self.coef = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.freq = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.phase = nn.Linear(2, hidden_dim//2, bias=False)        \n",
    "\n",
    "        self.imnet = make(imnet_spec, args={'in_dim': hidden_dim})\n",
    "\n",
    "    def gen_feat(self, inp):\n",
    "        self.inp = inp\n",
    "        self.feat_coord = make_coord(inp.shape[-2:], flatten=False).cuda() \\\n",
    "            .permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(inp.shape[0], 2, *inp.shape[-2:])\n",
    "        \n",
    "        self.feat = self.encoder(inp)\n",
    "        self.coeff = self.coef(self.feat)\n",
    "        self.freqq = self.freq(self.feat)\n",
    "        return self.feat\n",
    "\n",
    "    def query_rgb(self, coord, cell=None):\n",
    "        feat = self.feat\n",
    "        coef = self.coeff\n",
    "        freq = self.freqq\n",
    "\n",
    "        vx_lst = [-1, 1]\n",
    "        vy_lst = [-1, 1]\n",
    "        eps_shift = 1e-6 \n",
    "\n",
    "        # field radius (global: [-1, 1])\n",
    "        rx = 2 / feat.shape[-2] / 2\n",
    "        ry = 2 / feat.shape[-1] / 2\n",
    "\n",
    "        feat_coord = self.feat_coord\n",
    "\n",
    "        preds = []\n",
    "        areas = []\n",
    "        for vx in vx_lst:\n",
    "            for vy in vy_lst:\n",
    "                # prepare coefficient & frequency\n",
    "                coord_ = coord.clone()\n",
    "                coord_[:, :, 0] += vx * rx + eps_shift\n",
    "                coord_[:, :, 1] += vy * ry + eps_shift\n",
    "                coord_.clamp_(-1 + 1e-6, 1 - 1e-6)\n",
    "                q_coef = F.grid_sample(\n",
    "                    coef, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                q_freq = F.grid_sample(\n",
    "                    freq, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                q_coord = F.grid_sample(\n",
    "                    feat_coord, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                rel_coord = coord - q_coord\n",
    "                rel_coord[:, :, 0] *= feat.shape[-2]\n",
    "                rel_coord[:, :, 1] *= feat.shape[-1]\n",
    "                \n",
    "                # prepare cell\n",
    "                rel_cell = cell.clone()\n",
    "                rel_cell[:, :, 0] *= feat.shape[-2]\n",
    "                rel_cell[:, :, 1] *= feat.shape[-1]\n",
    "                \n",
    "                # basis generation\n",
    "                bs, q = coord.shape[:2]\n",
    "                q_freq = torch.stack(torch.split(q_freq, 2, dim=-1), dim=-1)\n",
    "                q_freq = torch.mul(q_freq, rel_coord.unsqueeze(-1))\n",
    "                q_freq = torch.sum(q_freq, dim=-2)\n",
    "                q_freq += self.phase(rel_cell.view((bs * q, -1))).view(bs, q, -1)\n",
    "                q_freq = torch.cat((torch.cos(np.pi*q_freq), torch.sin(np.pi*q_freq)), dim=-1)\n",
    "\n",
    "                inp = torch.mul(q_coef, q_freq)            \n",
    "\n",
    "                pred = self.imnet(inp.contiguous().view(bs * q, -1)).view(bs, q, -1)\n",
    "                preds.append(pred)\n",
    "\n",
    "                area = torch.abs(rel_coord[:, :, 0] * rel_coord[:, :, 1])\n",
    "                areas.append(area + 1e-9)\n",
    "\n",
    "        tot_area = torch.stack(areas).sum(dim=0)\n",
    "        t = areas[0]; areas[0] = areas[3]; areas[3] = t\n",
    "        t = areas[1]; areas[1] = areas[2]; areas[2] = t\n",
    "        \n",
    "        ret = 0\n",
    "        for pred, area in zip(preds, areas):\n",
    "            ret = ret + pred * (area / tot_area).unsqueeze(-1)\n",
    "        ret += F.grid_sample(self.inp, coord.flip(-1).unsqueeze(1), mode='bilinear',\\\n",
    "                      padding_mode='border', align_corners=False)[:, :, 0, :] \\\n",
    "                      .permute(0, 2, 1)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, inp, coord, cell):\n",
    "        self.gen_feat(inp)\n",
    "        return self.query_rgb(coord, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('mlp')\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, hidden_list):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        lastv = in_dim\n",
    "        for hidden in hidden_list:\n",
    "            layers.append(nn.Linear(lastv, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            lastv = hidden\n",
    "        layers.append(nn.Linear(lastv, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.layers(x.view(-1, x.shape[-1]))\n",
    "        return x.view(*shape, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0.0\n",
    "        self.v = 0.0\n",
    "\n",
    "    def add(self, v, n=1.0):\n",
    "        self.v = (self.v * self.n + v * n) / (self.n + n)\n",
    "        self.n += n\n",
    "\n",
    "    def item(self):\n",
    "        return self.v\n",
    "\n",
    "\n",
    "class Timer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.v = time.time()\n",
    "\n",
    "    def s(self):\n",
    "        self.v = time.time()\n",
    "\n",
    "    def t(self):\n",
    "        return time.time() - self.v\n",
    "\n",
    "\n",
    "def time_text(t):\n",
    "    if t >= 3600:\n",
    "        return '{:.1f}h'.format(t / 3600)\n",
    "    elif t >= 60:\n",
    "        return '{:.1f}m'.format(t / 60)\n",
    "    else:\n",
    "        return '{:.1f}s'.format(t)\n",
    "\n",
    "\n",
    "_log_path = None\n",
    "\n",
    "\n",
    "def set_log_path(path):\n",
    "    global _log_path\n",
    "    _log_path = path\n",
    "\n",
    "\n",
    "def log(obj, filename='log.txt'):\n",
    "    print(obj)\n",
    "    if _log_path is not None:\n",
    "        with open(os.path.join(_log_path, filename), 'a') as f:\n",
    "            print(obj, file=f)\n",
    "\n",
    "\n",
    "def ensure_path(path, remove=True):\n",
    "    basename = os.path.basename(path.rstrip('/'))\n",
    "    if os.path.exists(path):\n",
    "        if remove and (basename.startswith('_')\n",
    "                or input('{} exists, remove? (y/[n]): '.format(path)) == 'y'):\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def set_save_path(save_path, remove=True):\n",
    "    ensure_path(save_path, remove=remove)\n",
    "    set_log_path(save_path)\n",
    "    writer = SummaryWriter(os.path.join(save_path, 'tensorboard'))\n",
    "    return log, writer\n",
    "\n",
    "\n",
    "def compute_num_params(model, text=False):\n",
    "    tot = int(sum([np.prod(p.shape) for p in model.parameters()]))\n",
    "    if text:\n",
    "        if tot >= 1e6:\n",
    "            return '{:.1f}M'.format(tot / 1e6)\n",
    "        else:\n",
    "            return '{:.1f}K'.format(tot / 1e3)\n",
    "    else:\n",
    "        return tot\n",
    "\n",
    "\n",
    "def make_optimizer(param_list, optimizer_spec, load_sd=False):\n",
    "    Optimizer = {\n",
    "        'sgd': SGD,\n",
    "        'adam': Adam\n",
    "    }[optimizer_spec['name']]\n",
    "    optimizer = Optimizer(param_list, **optimizer_spec['args'])\n",
    "    if load_sd:\n",
    "        optimizer.load_state_dict(optimizer_spec['sd'])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    \"\"\" Make coordinates at grid centers.\n",
    "    \"\"\"\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def to_pixel_samples(img):\n",
    "    \"\"\" Convert the image to coord-RGB pairs.\n",
    "        img: Tensor, (3, H, W)\n",
    "    \"\"\"\n",
    "    coord = make_coord(img.shape[-2:])\n",
    "    rgb = img.view(3, -1).permute(1, 0)\n",
    "    return coord, rgb\n",
    "\n",
    "\n",
    "def calc_psnr(sr, hr, dataset=None, scale=1, rgb_range=1):\n",
    "    diff = (sr - hr) / rgb_range\n",
    "    if dataset is not None:\n",
    "        if dataset == 'benchmark':\n",
    "            shave = scale\n",
    "            if diff.size(1) > 1:\n",
    "                gray_coeffs = [65.738, 129.057, 25.064]\n",
    "                convert = diff.new_tensor(gray_coeffs).view(1, 3, 1, 1) / 256\n",
    "                diff = diff.mul(convert).sum(dim=1)\n",
    "        elif dataset == 'div2k':\n",
    "            shave = scale + 6\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        valid = diff[..., shave:-shave, shave:-shave]\n",
    "    else:\n",
    "        valid = diff\n",
    "    mse = valid.pow(2).mean()\n",
    "    return -10 * torch.log10(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "def register_dataset(name):\n",
    "    def decorator(cls):\n",
    "        datasets[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def make_dataset(dataset_spec, args=None):\n",
    "    if args is not None:\n",
    "        dataset_args = copy.deepcopy(dataset_spec['args'])\n",
    "        dataset_args.update(args)\n",
    "    else:\n",
    "        dataset_args = dataset_spec['args']\n",
    "    dataset = datasets[dataset_spec['name']](**dataset_args)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "@register_dataset('image-folder')\n",
    "class ImageFolder(Dataset):\n",
    "\n",
    "    def __init__(self, root_path, split_file=None, split_key=None, first_k=None,\n",
    "                 repeat=1, cache='none'):\n",
    "        self.repeat = repeat\n",
    "        self.cache = cache\n",
    "\n",
    "        if split_file is None:\n",
    "            filenames = sorted(os.listdir(root_path))\n",
    "        else:\n",
    "            with open(split_file, 'r') as f:\n",
    "                filenames = json.load(f)[split_key]\n",
    "        if first_k is not None:\n",
    "            filenames = filenames[:first_k]\n",
    "\n",
    "        self.files = []\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(root_path, filename)\n",
    "\n",
    "            if cache == 'none':\n",
    "                self.files.append(file)\n",
    "\n",
    "            elif cache == 'bin':\n",
    "                bin_root = os.path.join(os.path.dirname(root_path),\n",
    "                    '_bin_' + os.path.basename(root_path))\n",
    "                if not os.path.exists(bin_root):\n",
    "                    os.mkdir(bin_root)\n",
    "                    print('mkdir', bin_root)\n",
    "                bin_file = os.path.join(\n",
    "                    bin_root, filename.split('.')[0] + '.pkl')\n",
    "                if not os.path.exists(bin_file):\n",
    "                    with open(bin_file, 'wb') as f:\n",
    "                        pickle.dump(imageio.imread(file), f)\n",
    "                    print('dump', bin_file)\n",
    "                self.files.append(bin_file)\n",
    "\n",
    "            elif cache == 'in_memory':\n",
    "                self.files.append(transforms.ToTensor()(\n",
    "                    Image.open(file).convert('RGB')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files) * self.repeat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.files[idx % len(self.files)]\n",
    "\n",
    "        if self.cache == 'none':\n",
    "            return transforms.ToTensor()(Image.open(x).convert('RGB'))\n",
    "\n",
    "        elif self.cache == 'bin':\n",
    "            with open(x, 'rb') as f:\n",
    "                x = pickle.load(f)\n",
    "            x = np.ascontiguousarray(x.transpose(2, 0, 1))\n",
    "            x = torch.from_numpy(x).float() / 255\n",
    "            return x\n",
    "\n",
    "        elif self.cache == 'in_memory':\n",
    "            return x\n",
    "\n",
    "\n",
    "@register_dataset('paired-image-folders')\n",
    "class PairedImageFolders(Dataset):\n",
    "\n",
    "    def __init__(self, root_path_1, root_path_2, **kwargs):\n",
    "        self.dataset_1 = ImageFolder(root_path_1, **kwargs)\n",
    "        self.dataset_2 = ImageFolder(root_path_2, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_1[idx], self.dataset_2[idx]\n",
    "\n",
    "\n",
    "@register_dataset('sr-implicit-paired')\n",
    "class SRImplicitPaired(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, inp_size=None, augment=False, sample_q=None):\n",
    "        self.dataset = dataset\n",
    "        self.inp_size = inp_size\n",
    "        self.augment = augment\n",
    "        self.sample_q = sample_q\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_lr, img_hr = self.dataset[idx]\n",
    "\n",
    "        s = img_hr.shape[-2] // img_lr.shape[-2] # assume int scale\n",
    "        if self.inp_size is None:\n",
    "            h_lr, w_lr = img_lr.shape[-2:]\n",
    "            img_hr = img_hr[:, :h_lr * s, :w_lr * s]\n",
    "            crop_lr, crop_hr = img_lr, img_hr\n",
    "        else:\n",
    "            w_lr = self.inp_size\n",
    "            x0 = random.randint(0, img_lr.shape[-2] - w_lr)\n",
    "            y0 = random.randint(0, img_lr.shape[-1] - w_lr)\n",
    "            crop_lr = img_lr[:, x0: x0 + w_lr, y0: y0 + w_lr]\n",
    "            w_hr = w_lr * s\n",
    "            x1 = x0 * s\n",
    "            y1 = y0 * s\n",
    "            crop_hr = img_hr[:, x1: x1 + w_hr, y1: y1 + w_hr]\n",
    "\n",
    "        if self.augment:\n",
    "            hflip = random.random() < 0.5\n",
    "            vflip = random.random() < 0.5\n",
    "            dflip = random.random() < 0.5\n",
    "\n",
    "            def augment(x):\n",
    "                if hflip:\n",
    "                    x = x.flip(-2)\n",
    "                if vflip:\n",
    "                    x = x.flip(-1)\n",
    "                if dflip:\n",
    "                    x = x.transpose(-2, -1)\n",
    "                return x\n",
    "\n",
    "            crop_lr = augment(crop_lr)\n",
    "            crop_hr = augment(crop_hr)\n",
    "\n",
    "        hr_coord, hr_rgb = to_pixel_samples(crop_hr.contiguous())\n",
    "\n",
    "        if self.sample_q is not None:\n",
    "            sample_lst = np.random.choice(\n",
    "                len(hr_coord), self.sample_q, replace=False)\n",
    "            hr_coord = hr_coord[sample_lst]\n",
    "            hr_rgb = hr_rgb[sample_lst]\n",
    "\n",
    "        cell = torch.ones_like(hr_coord)\n",
    "        cell[:, 0] *= 2 / crop_hr.shape[-2]\n",
    "        cell[:, 1] *= 2 / crop_hr.shape[-1]\n",
    "\n",
    "        return {\n",
    "            'inp': crop_lr,\n",
    "            'coord': hr_coord,\n",
    "            'cell': cell,\n",
    "            'gt': hr_rgb\n",
    "        }\n",
    "\n",
    "def resize_fn(img, size):\n",
    "    return transforms.ToTensor()(\n",
    "        transforms.Resize(size, Image.BICUBIC)(\n",
    "            transforms.ToPILImage()(img)))\n",
    "\n",
    "\n",
    "@register_dataset('sr-implicit-downsampled')\n",
    "class SRImplicitDownsampled(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, inp_size=None, scale_min=1, scale_max=None,\n",
    "                 augment=False, sample_q=None):\n",
    "        self.dataset = dataset\n",
    "        self.inp_size = inp_size\n",
    "        self.scale_min = scale_min\n",
    "        if scale_max is None:\n",
    "            scale_max = scale_min\n",
    "        self.scale_max = scale_max\n",
    "        self.augment = augment\n",
    "        self.sample_q = sample_q\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx]\n",
    "        s = random.uniform(self.scale_min, self.scale_max)\n",
    "\n",
    "        if self.inp_size is None:\n",
    "            h_lr = math.floor(img.shape[-2] / s + 1e-9)\n",
    "            w_lr = math.floor(img.shape[-1] / s + 1e-9)\n",
    "            img = img[:, :round(h_lr * s), :round(w_lr * s)] # assume round int\n",
    "            img_down = resize_fn(img, (h_lr, w_lr))\n",
    "            crop_lr, crop_hr = img_down, img\n",
    "        else:\n",
    "            w_lr = self.inp_size\n",
    "            w_hr = round(w_lr * s)\n",
    "            x0 = random.randint(0, img.shape[-2] - w_hr)\n",
    "            y0 = random.randint(0, img.shape[-1] - w_hr)\n",
    "            crop_hr = img[:, x0: x0 + w_hr, y0: y0 + w_hr]\n",
    "            crop_lr = resize_fn(crop_hr, w_lr)\n",
    "\n",
    "        if self.augment:\n",
    "            hflip = random.random() < 0.5\n",
    "            vflip = random.random() < 0.5\n",
    "            dflip = random.random() < 0.5\n",
    "\n",
    "            def augment(x):\n",
    "                if hflip:\n",
    "                    x = x.flip(-2)\n",
    "                if vflip:\n",
    "                    x = x.flip(-1)\n",
    "                if dflip:\n",
    "                    x = x.transpose(-2, -1)\n",
    "                return x\n",
    "\n",
    "            crop_lr = augment(crop_lr)\n",
    "            crop_hr = augment(crop_hr)\n",
    "\n",
    "        hr_coord, hr_rgb = to_pixel_samples(crop_hr.contiguous())\n",
    "\n",
    "        if self.sample_q is not None:\n",
    "            sample_lst = np.random.choice(\n",
    "                len(hr_coord), self.sample_q, replace=False)\n",
    "            hr_coord = hr_coord[sample_lst]\n",
    "            hr_rgb = hr_rgb[sample_lst]\n",
    "\n",
    "        cell = torch.ones_like(hr_coord)\n",
    "        cell[:, 0] *= 2 / crop_hr.shape[-2]\n",
    "        cell[:, 1] *= 2 / crop_hr.shape[-1]\n",
    "\n",
    "        return {\n",
    "            'inp': crop_lr,\n",
    "            'coord': hr_coord,\n",
    "            'cell': cell,\n",
    "            'gt': hr_rgb\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_predict(model, inp, coord, cell, bsize):\n",
    "    with torch.no_grad():\n",
    "        model.gen_feat(inp)\n",
    "        n = coord.shape[1]\n",
    "        ql = 0\n",
    "        preds = []\n",
    "        while ql < n:\n",
    "            qr = min(ql + bsize, n)\n",
    "            pred = model.query_rgb(coord[:, ql: qr, :], cell[:, ql: qr, :])\n",
    "            preds.append(pred)\n",
    "            ql = qr\n",
    "        pred = torch.cat(preds, dim=1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval_psnr(loader, model, data_norm=None, eval_type=None, eval_bsize=None, window_size=0, scale_max=4, fast=False,\n",
    "              verbose=False):\n",
    "    model.eval()\n",
    "\n",
    "    if data_norm is None:\n",
    "        data_norm = {\n",
    "            'inp': {'sub': [0], 'div': [1]},\n",
    "            'gt': {'sub': [0], 'div': [1]}\n",
    "        }\n",
    "    t = data_norm['inp']\n",
    "    inp_sub = torch.FloatTensor(t['sub']).view(1, -1, 1, 1).cuda()\n",
    "    inp_div = torch.FloatTensor(t['div']).view(1, -1, 1, 1).cuda()\n",
    "    t = data_norm['gt']\n",
    "    gt_sub = torch.FloatTensor(t['sub']).view(1, 1, -1).cuda()\n",
    "    gt_div = torch.FloatTensor(t['div']).view(1, 1, -1).cuda()\n",
    "\n",
    "    if eval_type is None:\n",
    "        metric_fn = calc_psnr\n",
    "    elif eval_type.startswith('div2k'):\n",
    "        scale = int(eval_type.split('-')[1])\n",
    "        metric_fn = partial(calc_psnr, dataset='div2k', scale=scale)\n",
    "    elif eval_type.startswith('benchmark'):\n",
    "        scale = int(eval_type.split('-')[1])\n",
    "        metric_fn = partial(calc_psnr, dataset='benchmark', scale=scale)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    val_res = Averager()\n",
    "\n",
    "    pbar = tqdm(loader, leave=False, desc='val')\n",
    "    for batch in pbar:\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.cuda()\n",
    "\n",
    "        inp = (batch['inp'] - inp_sub) / inp_div\n",
    "        # SwinIR Evaluation - reflection padding\n",
    "        if window_size != 0:\n",
    "            _, _, h_old, w_old = inp.size()\n",
    "            h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "            w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "            inp = torch.cat([inp, torch.flip(inp, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "            inp = torch.cat([inp, torch.flip(inp, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "            \n",
    "            coord = make_coord((scale*(h_old+h_pad), scale*(w_old+w_pad))).unsqueeze(0).cuda()\n",
    "            cell = torch.ones_like(coord)\n",
    "            cell[:, :, 0] *= 2 / inp.shape[-2] / scale\n",
    "            cell[:, :, 1] *= 2 / inp.shape[-1] / scale\n",
    "        else:\n",
    "            h_pad = 0\n",
    "            w_pad = 0\n",
    "            \n",
    "            coord = batch['coord']\n",
    "            cell = batch['cell']\n",
    "            \n",
    "        if eval_bsize is None:\n",
    "            with torch.no_grad():\n",
    "                pred = model(inp, coord, cell)\n",
    "        else:\n",
    "            if fast:\n",
    "                pred = model(inp, coord, cell*max(scale/scale_max, 1))\n",
    "            else:\n",
    "                pred = batched_predict(model, inp, coord, cell*max(scale/scale_max, 1), eval_bsize) # cell clip for extrapolation\n",
    "            \n",
    "        pred = pred * gt_div + gt_sub\n",
    "        pred.clamp_(0, 1)\n",
    "\n",
    "        if eval_type is not None and fast == False: # reshape for shaving-eval\n",
    "            # gt reshape\n",
    "            ih, iw = batch['inp'].shape[-2:]\n",
    "            s = math.sqrt(batch['coord'].shape[1] / (ih * iw))\n",
    "            shape = [batch['inp'].shape[0], round(ih * s), round(iw * s), 3]\n",
    "            batch['gt'] = batch['gt'].view(*shape) \\\n",
    "                .permute(0, 3, 1, 2).contiguous()\n",
    "            \n",
    "            # prediction reshape\n",
    "            ih += h_pad\n",
    "            iw += w_pad\n",
    "            s = math.sqrt(coord.shape[1] / (ih * iw))\n",
    "            shape = [batch['inp'].shape[0], round(ih * s), round(iw * s), 3]\n",
    "            pred = pred.view(*shape) \\\n",
    "                .permute(0, 3, 1, 2).contiguous()\n",
    "            pred = pred[..., :batch['gt'].shape[-2], :batch['gt'].shape[-1]]\n",
    "            \n",
    "        res = metric_fn(pred, batch['gt'])\n",
    "        val_res.add(res.item(), inp.shape[0])\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description('val {:.4f}'.format(val_res.item()))\n",
    "            \n",
    "    return val_res.item()\n",
    "\n",
    "def test(config, model, window=0, scale_max=4, fast=False, gpu=0):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "    with open(config, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    spec = config['test_dataset']\n",
    "    dataset = make_dataset(spec['dataset'])\n",
    "    dataset = make_dataset(spec['wrapper'], args={'dataset': dataset})\n",
    "    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n",
    "        num_workers=1, pin_memory=True)\n",
    "\n",
    "    model_spec = torch.load(model)['model']\n",
    "    model = make(model_spec, load_sd=True).cuda()\n",
    "\n",
    "    res = eval_psnr(loader, model,\n",
    "        data_norm=config.get('data_norm'),\n",
    "        eval_type=config.get('eval_type'),\n",
    "        eval_bsize=config.get('eval_bsize'),\n",
    "        window_size=int(window),\n",
    "        scale_max = int(scale_max),\n",
    "        fast = fast,\n",
    "        verbose=True)\n",
    "    print('result: {:.4f}'.format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Applications\\Anaconda\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "                                            \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13068, 24436, 31212, 13728, 27400, 10044, 18288, 23428) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d908e7c9a750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'configs/test/test-div2k-2.yaml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'save/SwinIR_epoch-last.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-6802359a2632>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(config, model, window, scale_max, fast, gpu)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_sd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     res = eval_psnr(loader, model,\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mdata_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_norm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0meval_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'eval_type'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6802359a2632>\u001b[0m in \u001b[0;36meval_psnr\u001b[1;34m(loader, model, data_norm, eval_type, eval_bsize, window_size, scale_max, fast, verbose)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Applications\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13068, 24436, 31212, 13728, 27400, 10044, 18288, 23428) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "test(config='configs/test/test-div2k-2.yaml', model='save/SwinIR_epoch-last.pth', window=8, gpu='0' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce7721647274386fe13b7f2220e2cccc8d7a3ea400a265b5456a0f9751e59dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
