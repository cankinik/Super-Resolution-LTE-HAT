{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file combines everything in the project into a single view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that CUDA is available, and import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.12.1\n",
      "Torchvision Version:  0.13.1\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import math\n",
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from torch.optim import SGD, Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep track of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "\n",
    "def register(name):\n",
    "    def decorator(cls):\n",
    "        models[name] = cls\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def make(model_spec, args=None, load_sd=False):\n",
    "    if args is not None:\n",
    "        model_args = copy.deepcopy(model_spec['args'])\n",
    "        model_args.update(args)\n",
    "    else:\n",
    "        model_args = model_spec['args']\n",
    "    model = models[model_spec['name']](**model_args)\n",
    "    if load_sd:\n",
    "        model.load_state_dict(model_spec['sd'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinIR Version (Run this rather than the cell after if you want SwinIR as Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        H, W = x_size\n",
    "        B, L, C = x.shape\n",
    "        # assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        if self.input_resolution == x_size:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, x_size)\n",
    "            else:\n",
    "                x = blk(x, x_size)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 img_size=224, patch_size=4, resi_connection='1conv'):\n",
    "        super(RSTB, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.residual_group = BasicLayer(dim=dim,\n",
    "                                         input_resolution=input_resolution,\n",
    "                                         depth=depth,\n",
    "                                         num_heads=num_heads,\n",
    "                                         window_size=window_size,\n",
    "                                         mlp_ratio=mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                         drop=drop, attn_drop=attn_drop,\n",
    "                                         drop_path=drop_path,\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         downsample=downsample,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n",
    "                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.residual_group.flops()\n",
    "        H, W = self.input_resolution\n",
    "        flops += H * W * self.dim * self.dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        flops += self.patch_unembed.flops()\n",
    "\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.img_size\n",
    "        if self.norm is not None:\n",
    "            flops += H * W * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "class UpsampleOneStep(nn.Sequential):\n",
    "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
    "       Used in lightweight SR to save parameters.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
    "        self.num_feat = num_feat\n",
    "        self.input_resolution = input_resolution\n",
    "        m = []\n",
    "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "        super(UpsampleOneStep, self).__init__(*m)\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.num_feat * 3 * 9\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    r\"\"\" SwinIR\n",
    "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=48, patch_size=1, in_chans=3,\n",
    "                #  embed_dim=180, depths=[6, 6, 6, 6, 6, 6], num_heads=[6, 6, 6, 6, 6, 6],              # Trying the full model to see how it fares\n",
    "                 embed_dim=180, depths=[6, 6, 6], num_heads=[6, 6, 6],            # Making the model smaller                \n",
    "                 window_size=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, upscale=2, img_range=1., upsampler='none', resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(SwinIR, self).__init__()\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "        self.window_size = window_size\n",
    "        self.out_dim = num_feat\n",
    "        #####################################################################################################\n",
    "        ################################### 1, shallow feature extraction ###################################\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################### 2, deep feature extraction ######################################\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build Residual Swin Transformer blocks (RSTB)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RSTB(dim=embed_dim,\n",
    "                         input_resolution=(patches_resolution[0],\n",
    "                                           patches_resolution[1]),\n",
    "                         depth=depths[i_layer],\n",
    "                         num_heads=num_heads[i_layer],\n",
    "                         window_size=window_size,\n",
    "                         mlp_ratio=self.mlp_ratio,\n",
    "                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                         norm_layer=norm_layer,\n",
    "                         downsample=None,\n",
    "                         use_checkpoint=use_checkpoint,\n",
    "                         img_size=img_size,\n",
    "                         patch_size=patch_size,\n",
    "                         resi_connection=resi_connection\n",
    "\n",
    "                         )\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################ 3, high quality image reconstruction ################################\n",
    "        if self.upsampler == 'none':\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))        \n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.upsample = Upsample(upscale, num_feat)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR (to save parameters)\n",
    "            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n",
    "                                            (patches_resolution[0], patches_resolution[1]))\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR (less artifacts)\n",
    "            assert self.upscale == 4, 'only support x4 now.'\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x_size)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.patch_unembed(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.check_image_size(x)\n",
    "        \n",
    "#         self.mean = self.mean.type_as(x)\n",
    "#         x = (x - self.mean) * self.img_range\n",
    "        \n",
    "        if self.upsampler == 'none':\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.conv_last(self.upsample(x))\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.upsample(x)\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            x_first = self.conv_first(x)\n",
    "            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n",
    "            x = x + self.conv_last(res)\n",
    "\n",
    "#         x = x / self.img_range + self.mean\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.patches_resolution\n",
    "        flops += H * W * 3 * self.embed_dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += H * W * 3 * self.embed_dim * self.embed_dim\n",
    "        flops += self.upsample.flops()\n",
    "        return flops\n",
    "\n",
    "@register('swinir')\n",
    "def make_swinir(no_upsampling=True):\n",
    "    return SwinIR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAT Version (Run this rather than the cell prior if you want HAT as Encoder) Note that names still seems as if it is SwinIR, but it is modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "\n",
    "        # We should be calculating this separately now, since OCAB is different than HAB. Hence, pass rpi as argument to the forward function\n",
    "        # # get pair-wise relative position index for each token inside the window\n",
    "        # coords_h = torch.arange(self.window_size[0])\n",
    "        # coords_w = torch.arange(self.window_size[1])\n",
    "        # coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        # coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        # relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        # relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        # relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        # relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        # relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        # relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        # self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, \n",
    "        # The forward function should get the rpi and not use self.relative_position_index.view(-1)\n",
    "        rpi, \n",
    "        mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        # Changed it so that it is using  the rpi from the argument, as it will be different for OCAB vs HAB\n",
    "        # relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "# Used by the CAB module\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention used in RCAN.\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        squeeze_factor (int): Channel squeeze factor. Default: 16.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat, squeeze_factor=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        return x * y\n",
    "\n",
    "# HAT uses a CAB module inside what is the equivalent of the SwinTransformerBlock\n",
    "class CAB(nn.Module):\n",
    "\n",
    "    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n",
    "        super(CAB, self).__init__()\n",
    "\n",
    "        self.cab = nn.Sequential(\n",
    "            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n",
    "            ChannelAttention(num_feat, squeeze_factor)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cab(x)\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 # These arguments are required by the CAB module and the related operations\n",
    "                 conv_scale=0.01,\n",
    "                 compress_ratio=3,\n",
    "                 squeeze_factor=30\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        # Initialize the CAB module and the conv scale\n",
    "        self.conv_scale = conv_scale\n",
    "        self.conv_block = CAB(num_feat=dim, compress_ratio=compress_ratio, squeeze_factor=squeeze_factor)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        # Don't calculate mask here, take it as an argument\n",
    "        # if self.shift_size > 0:\n",
    "        #     attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        # else:\n",
    "        #     attn_mask = None\n",
    "\n",
    "        # self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        H, W = x_size\n",
    "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, x_size,\n",
    "        # Required now, as they are also used by the forward of HAB\n",
    "        rpi_sa, attn_mask\n",
    "    ):\n",
    "        H, W = x_size\n",
    "        B, L, C = x.shape\n",
    "        # assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # Conv_X: Addition from the HAT side that utilizes the CAB mdule\n",
    "        conv_x = self.conv_block(x.permute(0, 3, 1, 2))\n",
    "        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        # if self.input_resolution == x_size:\n",
    "            # attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "        # else:\n",
    "            # Use the given rpi and the mask\n",
    "            # attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n",
    "\n",
    "        # Use the given rpi and the mask\n",
    "        attn_windows = self.attn(x_windows, rpi=rpi_sa, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x += conv_x * self.conv_scale                   # This is the contribution from the CAB side as per HAT\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                # The overlap ratio is used by the OCAB block, which comes from the HAT architecture\n",
    "                 overlap_ratio,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # OCAB: From HAT: STL's have been changed to be equivalent to HAB's, and then we add the OCAB layer to the end\n",
    "        self.overlap_attn = OCAB(\n",
    "                            dim=dim,\n",
    "                            input_resolution=input_resolution,\n",
    "                            window_size=window_size,\n",
    "                            overlap_ratio=overlap_ratio,\n",
    "                            num_heads=num_heads,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            qk_scale=qk_scale,\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            norm_layer=norm_layer\n",
    "                            )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size\n",
    "        # Passing params to get rpi and mask\n",
    "        , params\n",
    "        ):\n",
    "        # The main change is that it will also run the data through OCAB, and also take in the parameters for the rpi and mask\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, x_size, params['rpi_sa'], params['attn_mask'])\n",
    "            else:\n",
    "                x = blk(x, x_size, params['rpi_sa'], params['attn_mask'])\n",
    "\n",
    "        # Also pass through the OCAB layer\n",
    "        # x = self.overlap_attn(x, x_size, params['rpi_oca'])\n",
    "        # We don't have params, but the corresponding value from the dictionary should be self.relative_position_index_OCA in HAT\n",
    "        #         relative_position_index_OCA = self.calculate_rpi_oca() -> \n",
    "        x = self.overlap_attn(x, x_size, params['rpi_oca'])\n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class RSTB(nn.Module):\n",
    "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 # To be used in the basic layer, which is the equivalent of AttenBlocks\n",
    "                 overlap_ratio,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 img_size=224, patch_size=4, resi_connection='1conv'):\n",
    "        super(RSTB, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # Basic layer should be transformed into the equivalent of the AttenBlocks over in HAT\n",
    "        self.residual_group = BasicLayer(dim=dim,\n",
    "                                         input_resolution=input_resolution,\n",
    "                                         depth=depth,\n",
    "                                         num_heads=num_heads,\n",
    "                                         window_size=window_size,\n",
    "                                         overlap_ratio=overlap_ratio,   # Getting the argument from parent class arguments, and passing to OCAB\n",
    "                                         mlp_ratio=mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                         drop=drop, attn_drop=attn_drop,\n",
    "                                         drop_path=drop_path,\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         downsample=downsample,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n",
    "                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n",
    "        # From HAT (RHAG)\n",
    "        elif resi_connection == 'identity':\n",
    "            self.conv = nn.Identity()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n",
    "            norm_layer=None)\n",
    "\n",
    "    def forward(self, x, x_size\n",
    "        # Also take in params\n",
    "        , params\n",
    "        ):\n",
    "        # Pass the params to the residual_group\n",
    "        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size, params), x_size))) + x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.residual_group.flops()\n",
    "        H, W = self.input_resolution\n",
    "        flops += H * W * self.dim * self.dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        flops += self.patch_unembed.flops()\n",
    "\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.img_size\n",
    "        if self.norm is not None:\n",
    "            flops += H * W * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "class UpsampleOneStep(nn.Sequential):\n",
    "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
    "       Used in lightweight SR to save parameters.\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
    "        self.num_feat = num_feat\n",
    "        self.input_resolution = input_resolution\n",
    "        m = []\n",
    "        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "        super(UpsampleOneStep, self).__init__(*m)\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.num_feat * 3 * 9\n",
    "        return flops\n",
    "\n",
    "\n",
    "# From the HAT. This goes to the end of the RSTB block, which is the equivalent of the RHAG block over at hat.\n",
    "class OCAB(nn.Module):\n",
    "    # overlapping cross-attention block\n",
    "\n",
    "    def __init__(self, dim,\n",
    "                input_resolution,\n",
    "                window_size,\n",
    "                overlap_ratio,\n",
    "                num_heads,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                mlp_ratio=2,\n",
    "                norm_layer=nn.LayerNorm\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        self.overlap_win_size = int(window_size * overlap_ratio) + window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3,  bias=qkv_bias)\n",
    "        self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size-window_size)//2)\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.proj = nn.Linear(dim,dim)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU)\n",
    "\n",
    "    def forward(self, x, x_size, rpi):\n",
    "        h, w = x_size\n",
    "        b, _, c = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        qkv = self.qkv(x).reshape(b, h, w, 3, c).permute(3, 0, 4, 1, 2) # 3, b, c, h, w\n",
    "        q = qkv[0].permute(0, 2, 3, 1) # b, h, w, c\n",
    "        kv = torch.cat((qkv[1], qkv[2]), dim=1) # b, 2*c, h, w\n",
    "\n",
    "        # partition windows\n",
    "        q_windows = window_partition(q, self.window_size)  # nw*b, window_size, window_size, c\n",
    "        q_windows = q_windows.view(-1, self.window_size * self.window_size, c)  # nw*b, window_size*window_size, c\n",
    "\n",
    "        kv_windows = self.unfold(kv) # b, c*w*w, nw\n",
    "        kv_windows = rearrange(kv_windows, 'b (nc ch owh oww) nw -> nc (b nw) (owh oww) ch', nc=2, ch=c, owh=self.overlap_win_size, oww=self.overlap_win_size).contiguous() # 2, nw*b, ow*ow, c\n",
    "        k_windows, v_windows = kv_windows[0], kv_windows[1] # nw*b, ow*ow, c\n",
    "\n",
    "        b_, nq, _ = q_windows.shape\n",
    "        _, n, _ = k_windows.shape\n",
    "        d = self.dim // self.num_heads\n",
    "        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, nq, d\n",
    "        k = k_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d\n",
    "        v = v_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3) # nw*b, nH, n, d\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size * self.window_size, self.overlap_win_size * self.overlap_win_size, -1)  # ws*ws, wse*wse, nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, ws*ws, wse*wse\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.dim)\n",
    "        x = window_reverse(attn_windows, self.window_size, h, w)  # b h w c\n",
    "        x = x.view(b, h * w, self.dim)\n",
    "\n",
    "        x = self.proj(x) + shortcut\n",
    "\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class SwinIR(nn.Module):\n",
    "    r\"\"\" SwinIR\n",
    "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=48, patch_size=1, in_chans=3,\n",
    "                # Adding overlap_ratio for the OCAB block, and changing the size of the model to be smaller\n",
    "                #  embed_dim=180, depths=[6, 6, 6, 6, 6, 6], num_heads=[6, 6, 6, 6, 6, 6], overlap_ratio=0.5,             # Trying the full model to see how it fares\n",
    "                 embed_dim=180, depths=[6, 6, 6], num_heads=[6, 6, 6], overlap_ratio=0.5,                                 # Making the model smaller\n",
    "                 window_size=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, upscale=2, img_range=1., upsampler='none', resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(SwinIR, self).__init__()\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "        self.window_size = window_size\n",
    "        self.out_dim = num_feat\n",
    "\n",
    "        # Need these here so that rpi calculation can use them\n",
    "        self.shift_size = window_size // 2\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        # From HAT: we calculate the rpi's over here, which will be passed inside the params dictionary given to the forward function\n",
    "        relative_position_index_SA = self.calculate_rpi_sa()\n",
    "        relative_position_index_OCA = self.calculate_rpi_oca()\n",
    "        self.register_buffer('relative_position_index_SA', relative_position_index_SA)\n",
    "        self.register_buffer('relative_position_index_OCA', relative_position_index_OCA)\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################### 1, shallow feature extraction ###################################\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################### 2, deep feature extraction ######################################\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build Residual Swin Transformer blocks (RSTB)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RSTB(dim=embed_dim,\n",
    "                         input_resolution=(patches_resolution[0],\n",
    "                                           patches_resolution[1]),\n",
    "                         depth=depths[i_layer],\n",
    "                         num_heads=num_heads[i_layer],\n",
    "                         window_size=window_size,\n",
    "                         # From the HAT side, we use the overlap ratio inside OCAB stuff``\n",
    "                         overlap_ratio=overlap_ratio,\n",
    "                         mlp_ratio=self.mlp_ratio,\n",
    "                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                         drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                         norm_layer=norm_layer,\n",
    "                         downsample=None,\n",
    "                         use_checkpoint=use_checkpoint,\n",
    "                         img_size=img_size,\n",
    "                         patch_size=patch_size,\n",
    "                         resi_connection=resi_connection\n",
    "\n",
    "                         )\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n",
    "                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################ 3, high quality image reconstruction ################################\n",
    "        if self.upsampler == 'none':\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))        \n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.upsample = Upsample(upscale, num_feat)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR (to save parameters)\n",
    "            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n",
    "                                            (patches_resolution[0], patches_resolution[1]))\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR (less artifacts)\n",
    "            assert self.upscale == 4, 'only support x4 now.'\n",
    "            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n",
    "                                                      nn.LeakyReLU(inplace=True))\n",
    "            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    # These three functions are from HAT, and they are moved here because we calculate different rpi and mask depending on whether we are using HAB or OCAB\n",
    "    def calculate_rpi_sa(self):\n",
    "        # calculate relative position index for SA\n",
    "        coords_h = torch.arange(self.window_size)\n",
    "        coords_w = torch.arange(self.window_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_rpi_oca(self):\n",
    "        # calculate relative position index for OCA\n",
    "        window_size_ori = self.window_size\n",
    "        window_size_ext = self.window_size + int(self.overlap_ratio * self.window_size)\n",
    "\n",
    "        coords_h = torch.arange(window_size_ori)\n",
    "        coords_w = torch.arange(window_size_ori)\n",
    "        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws\n",
    "        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws\n",
    "\n",
    "        coords_h = torch.arange(window_size_ext)\n",
    "        coords_w = torch.arange(window_size_ext)\n",
    "        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse\n",
    "        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse\n",
    "\n",
    "        relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]   # 2, ws*ws, wse*wse\n",
    "\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2\n",
    "        relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1\n",
    "\n",
    "        relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        h, w = x_size\n",
    "        img_mask = torch.zeros((1, h, w, 1))  # 1 h w 1\n",
    "        h_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size), slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size), slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nw, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "\n",
    "        # Added from the HAT class\n",
    "        # Calculate attention mask and relative position index in advance to speed up inference. \n",
    "        # The original code is very time-cosuming for large window size.\n",
    "        attn_mask = self.calculate_mask(x_size).to(x.device)\n",
    "        params = {'attn_mask': attn_mask, 'rpi_sa': self.relative_position_index_SA, 'rpi_oca': self.relative_position_index_OCA}\n",
    "\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # We are passing params now\n",
    "            x = layer(x, x_size, params)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.patch_unembed(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.check_image_size(x)\n",
    "        \n",
    "#         self.mean = self.mean.type_as(x)\n",
    "#         x = (x - self.mean) * self.img_range\n",
    "        \n",
    "        if self.upsampler == 'none':\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "        elif self.upsampler == 'pixelshuffle':\n",
    "            # for classical SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.conv_last(self.upsample(x))\n",
    "        elif self.upsampler == 'pixelshuffledirect':\n",
    "            # for lightweight SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.upsample(x)\n",
    "        elif self.upsampler == 'nearest+conv':\n",
    "            # for real-world SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n",
    "            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n",
    "        else:\n",
    "            # for image denoising and JPEG compression artifact reduction\n",
    "            x_first = self.conv_first(x)\n",
    "            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n",
    "            x = x + self.conv_last(res)\n",
    "\n",
    "#         x = x / self.img_range + self.mean\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.patches_resolution\n",
    "        flops += H * W * 3 * self.embed_dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += H * W * 3 * self.embed_dim * self.embed_dim\n",
    "        flops += self.upsample.flops()\n",
    "        return flops\n",
    "\n",
    "@register('swinir')\n",
    "def make_swinir(no_upsampling=True):\n",
    "    return SwinIR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('lte')\n",
    "class LTE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_spec, imnet_spec=None, hidden_dim=256):\n",
    "        super().__init__()        \n",
    "        self.encoder = make(encoder_spec)\n",
    "        self.coef = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.freq = nn.Conv2d(self.encoder.out_dim, hidden_dim, 3, padding=1)\n",
    "        self.phase = nn.Linear(2, hidden_dim//2, bias=False)        \n",
    "\n",
    "        self.imnet = make(imnet_spec, args={'in_dim': hidden_dim})\n",
    "\n",
    "    def gen_feat(self, inp):\n",
    "        self.inp = inp\n",
    "        self.feat_coord = make_coord(inp.shape[-2:], flatten=False).cuda() \\\n",
    "            .permute(2, 0, 1) \\\n",
    "            .unsqueeze(0).expand(inp.shape[0], 2, *inp.shape[-2:])\n",
    "        \n",
    "        self.feat = self.encoder(inp)\n",
    "        self.coeff = self.coef(self.feat)\n",
    "        self.freqq = self.freq(self.feat)\n",
    "        return self.feat\n",
    "\n",
    "    def query_rgb(self, coord, cell=None):\n",
    "        feat = self.feat\n",
    "        coef = self.coeff\n",
    "        freq = self.freqq\n",
    "\n",
    "        vx_lst = [-1, 1]\n",
    "        vy_lst = [-1, 1]\n",
    "        eps_shift = 1e-6 \n",
    "\n",
    "        # field radius (global: [-1, 1])\n",
    "        rx = 2 / feat.shape[-2] / 2\n",
    "        ry = 2 / feat.shape[-1] / 2\n",
    "\n",
    "        feat_coord = self.feat_coord\n",
    "\n",
    "        preds = []\n",
    "        areas = []\n",
    "        for vx in vx_lst:\n",
    "            for vy in vy_lst:\n",
    "                # prepare coefficient & frequency\n",
    "                coord_ = coord.clone()\n",
    "                coord_[:, :, 0] += vx * rx + eps_shift\n",
    "                coord_[:, :, 1] += vy * ry + eps_shift\n",
    "                coord_.clamp_(-1 + 1e-6, 1 - 1e-6)\n",
    "                q_coef = F.grid_sample(\n",
    "                    coef, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                q_freq = F.grid_sample(\n",
    "                    freq, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                q_coord = F.grid_sample(\n",
    "                    feat_coord, coord_.flip(-1).unsqueeze(1),\n",
    "                    mode='nearest', align_corners=False)[:, :, 0, :] \\\n",
    "                    .permute(0, 2, 1)\n",
    "                rel_coord = coord - q_coord\n",
    "                rel_coord[:, :, 0] *= feat.shape[-2]\n",
    "                rel_coord[:, :, 1] *= feat.shape[-1]\n",
    "                \n",
    "                # prepare cell\n",
    "                rel_cell = cell.clone()\n",
    "                rel_cell[:, :, 0] *= feat.shape[-2]\n",
    "                rel_cell[:, :, 1] *= feat.shape[-1]\n",
    "                \n",
    "                # basis generation\n",
    "                bs, q = coord.shape[:2]\n",
    "                q_freq = torch.stack(torch.split(q_freq, 2, dim=-1), dim=-1)\n",
    "                q_freq = torch.mul(q_freq, rel_coord.unsqueeze(-1))\n",
    "                q_freq = torch.sum(q_freq, dim=-2)\n",
    "                q_freq += self.phase(rel_cell.view((bs * q, -1))).view(bs, q, -1)\n",
    "                q_freq = torch.cat((torch.cos(np.pi*q_freq), torch.sin(np.pi*q_freq)), dim=-1)\n",
    "\n",
    "                inp = torch.mul(q_coef, q_freq)            \n",
    "\n",
    "                pred = self.imnet(inp.contiguous().view(bs * q, -1)).view(bs, q, -1)\n",
    "                preds.append(pred)\n",
    "\n",
    "                area = torch.abs(rel_coord[:, :, 0] * rel_coord[:, :, 1])\n",
    "                areas.append(area + 1e-9)\n",
    "\n",
    "        tot_area = torch.stack(areas).sum(dim=0)\n",
    "        t = areas[0]; areas[0] = areas[3]; areas[3] = t\n",
    "        t = areas[1]; areas[1] = areas[2]; areas[2] = t\n",
    "        \n",
    "        ret = 0\n",
    "        for pred, area in zip(preds, areas):\n",
    "            ret = ret + pred * (area / tot_area).unsqueeze(-1)\n",
    "        ret += F.grid_sample(self.inp, coord.flip(-1).unsqueeze(1), mode='bilinear',\\\n",
    "                      padding_mode='border', align_corners=False)[:, :, 0, :] \\\n",
    "                      .permute(0, 2, 1)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, inp, coord, cell):\n",
    "        self.gen_feat(inp)\n",
    "        return self.query_rgb(coord, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('mlp')\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, hidden_list):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        lastv = in_dim\n",
    "        for hidden in hidden_list:\n",
    "            layers.append(nn.Linear(lastv, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            lastv = hidden\n",
    "        layers.append(nn.Linear(lastv, out_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.layers(x.view(-1, x.shape[-1]))\n",
    "        return x.view(*shape, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0.0\n",
    "        self.v = 0.0\n",
    "\n",
    "    def add(self, v, n=1.0):\n",
    "        self.v = (self.v * self.n + v * n) / (self.n + n)\n",
    "        self.n += n\n",
    "\n",
    "    def item(self):\n",
    "        return self.v\n",
    "\n",
    "\n",
    "class Timer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.v = time.time()\n",
    "\n",
    "    def s(self):\n",
    "        self.v = time.time()\n",
    "\n",
    "    def t(self):\n",
    "        return time.time() - self.v\n",
    "\n",
    "\n",
    "def time_text(t):\n",
    "    if t >= 3600:\n",
    "        return '{:.1f}h'.format(t / 3600)\n",
    "    elif t >= 60:\n",
    "        return '{:.1f}m'.format(t / 60)\n",
    "    else:\n",
    "        return '{:.1f}s'.format(t)\n",
    "\n",
    "\n",
    "_log_path = None\n",
    "\n",
    "\n",
    "def set_log_path(path):\n",
    "    global _log_path\n",
    "    _log_path = path\n",
    "\n",
    "\n",
    "def log(obj, filename='log.txt'):\n",
    "    print(obj)\n",
    "    if _log_path is not None:\n",
    "        with open(os.path.join(_log_path, filename), 'a') as f:\n",
    "            print(obj, file=f)\n",
    "\n",
    "\n",
    "def ensure_path(path, remove=True):\n",
    "    basename = os.path.basename(path.rstrip('/'))\n",
    "    if os.path.exists(path):\n",
    "        if remove and (basename.startswith('_')\n",
    "                or input('{} exists, remove? (y/[n]): '.format(path)) == 'y'):\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def set_save_path(save_path, remove=True):\n",
    "    ensure_path(save_path, remove=remove)\n",
    "    set_log_path(save_path)\n",
    "    writer = SummaryWriter(os.path.join(save_path, 'tensorboard'))\n",
    "    return log, writer\n",
    "\n",
    "\n",
    "def compute_num_params(model, text=False):\n",
    "    tot = int(sum([np.prod(p.shape) for p in model.parameters()]))\n",
    "    if text:\n",
    "        if tot >= 1e6:\n",
    "            return '{:.1f}M'.format(tot / 1e6)\n",
    "        else:\n",
    "            return '{:.1f}K'.format(tot / 1e3)\n",
    "    else:\n",
    "        return tot\n",
    "\n",
    "\n",
    "def make_optimizer(param_list, optimizer_spec, load_sd=False):\n",
    "    Optimizer = {\n",
    "        'sgd': SGD,\n",
    "        'adam': Adam\n",
    "    }[optimizer_spec['name']]\n",
    "    optimizer = Optimizer(param_list, **optimizer_spec['args'])\n",
    "    if load_sd:\n",
    "        optimizer.load_state_dict(optimizer_spec['sd'])\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def make_coord(shape, ranges=None, flatten=True):\n",
    "    \"\"\" Make coordinates at grid centers.\n",
    "    \"\"\"\n",
    "    coord_seqs = []\n",
    "    for i, n in enumerate(shape):\n",
    "        if ranges is None:\n",
    "            v0, v1 = -1, 1\n",
    "        else:\n",
    "            v0, v1 = ranges[i]\n",
    "        r = (v1 - v0) / (2 * n)\n",
    "        seq = v0 + r + (2 * r) * torch.arange(n).float()\n",
    "        coord_seqs.append(seq)\n",
    "    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n",
    "    if flatten:\n",
    "        ret = ret.view(-1, ret.shape[-1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def to_pixel_samples(img):\n",
    "    \"\"\" Convert the image to coord-RGB pairs.\n",
    "        img: Tensor, (3, H, W)\n",
    "    \"\"\"\n",
    "    coord = make_coord(img.shape[-2:])\n",
    "    rgb = img.view(3, -1).permute(1, 0)\n",
    "    return coord, rgb\n",
    "\n",
    "\n",
    "def calc_psnr(sr, hr, dataset=None, scale=1, rgb_range=1):\n",
    "    diff = (sr - hr) / rgb_range\n",
    "    if dataset is not None:\n",
    "        if dataset == 'benchmark':\n",
    "            shave = scale\n",
    "            if diff.size(1) > 1:\n",
    "                gray_coeffs = [65.738, 129.057, 25.064]\n",
    "                convert = diff.new_tensor(gray_coeffs).view(1, 3, 1, 1) / 256\n",
    "                diff = diff.mul(convert).sum(dim=1)\n",
    "        elif dataset == 'div2k':\n",
    "            shave = scale + 6\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        valid = diff[..., shave:-shave, shave:-shave]\n",
    "    else:\n",
    "        valid = diff\n",
    "    mse = valid.pow(2).mean()\n",
    "    return -10 * torch.log10(mse)\n",
    "\n",
    "\n",
    "def batched_predict(model, inp, coord, cell, bsize):\n",
    "    with torch.no_grad():\n",
    "        model.gen_feat(inp)\n",
    "        n = coord.shape[1]\n",
    "        ql = 0\n",
    "        preds = []\n",
    "        while ql < n:\n",
    "            qr = min(ql + bsize, n)\n",
    "            pred = model.query_rgb(coord[:, ql: qr, :], cell[:, ql: qr, :])\n",
    "            preds.append(pred)\n",
    "            ql = qr\n",
    "        pred = torch.cat(preds, dim=1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def eval_psnr(loader, model, data_norm=None, eval_type=None, eval_bsize=None, window_size=0, scale_max=4, fast=False,\n",
    "              verbose=False):\n",
    "    model.eval()\n",
    "\n",
    "    if data_norm is None:\n",
    "        data_norm = {\n",
    "            'inp': {'sub': [0], 'div': [1]},\n",
    "            'gt': {'sub': [0], 'div': [1]}\n",
    "        }\n",
    "    t = data_norm['inp']\n",
    "    inp_sub = torch.FloatTensor(t['sub']).view(1, -1, 1, 1).cuda()\n",
    "    inp_div = torch.FloatTensor(t['div']).view(1, -1, 1, 1).cuda()\n",
    "    t = data_norm['gt']\n",
    "    gt_sub = torch.FloatTensor(t['sub']).view(1, 1, -1).cuda()\n",
    "    gt_div = torch.FloatTensor(t['div']).view(1, 1, -1).cuda()\n",
    "\n",
    "    if eval_type is None:\n",
    "        metric_fn = calc_psnr\n",
    "    elif eval_type.startswith('div2k'):\n",
    "        scale = int(eval_type.split('-')[1])\n",
    "        metric_fn = partial(calc_psnr, dataset='div2k', scale=scale)\n",
    "    elif eval_type.startswith('benchmark'):\n",
    "        scale = int(eval_type.split('-')[1])\n",
    "        metric_fn = partial(calc_psnr, dataset='benchmark', scale=scale)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    val_res = Averager()\n",
    "\n",
    "    pbar = tqdm(loader, leave=False, desc='val')\n",
    "    for batch in pbar:\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.cuda()\n",
    "\n",
    "        inp = (batch['inp'] - inp_sub) / inp_div\n",
    "        # SwinIR Evaluation - reflection padding\n",
    "        if window_size != 0:\n",
    "            _, _, h_old, w_old = inp.size()\n",
    "            h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "            w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "            inp = torch.cat([inp, torch.flip(inp, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "            inp = torch.cat([inp, torch.flip(inp, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "            \n",
    "            coord = make_coord((scale*(h_old+h_pad), scale*(w_old+w_pad))).unsqueeze(0).cuda()\n",
    "            cell = torch.ones_like(coord)\n",
    "            cell[:, :, 0] *= 2 / inp.shape[-2] / scale\n",
    "            cell[:, :, 1] *= 2 / inp.shape[-1] / scale\n",
    "        else:\n",
    "            h_pad = 0\n",
    "            w_pad = 0\n",
    "            \n",
    "            coord = batch['coord']\n",
    "            cell = batch['cell']\n",
    "            \n",
    "        if eval_bsize is None:\n",
    "            with torch.no_grad():\n",
    "                pred = model(inp, coord, cell)\n",
    "        else:\n",
    "            if fast:\n",
    "                pred = model(inp, coord, cell*max(scale/scale_max, 1))\n",
    "            else:\n",
    "                pred = batched_predict(model, inp, coord, cell*max(scale/scale_max, 1), eval_bsize) # cell clip for extrapolation\n",
    "            \n",
    "        pred = pred * gt_div + gt_sub\n",
    "        pred.clamp_(0, 1)\n",
    "\n",
    "        if eval_type is not None and fast == False: # reshape for shaving-eval\n",
    "            # gt reshape\n",
    "            ih, iw = batch['inp'].shape[-2:]\n",
    "            s = math.sqrt(batch['coord'].shape[1] / (ih * iw))\n",
    "            shape = [batch['inp'].shape[0], round(ih * s), round(iw * s), 3]\n",
    "            batch['gt'] = batch['gt'].view(*shape) \\\n",
    "                .permute(0, 3, 1, 2).contiguous()\n",
    "            \n",
    "            # prediction reshape\n",
    "            ih += h_pad\n",
    "            iw += w_pad\n",
    "            s = math.sqrt(coord.shape[1] / (ih * iw))\n",
    "            shape = [batch['inp'].shape[0], round(ih * s), round(iw * s), 3]\n",
    "            pred = pred.view(*shape) \\\n",
    "                .permute(0, 3, 1, 2).contiguous()\n",
    "            pred = pred[..., :batch['gt'].shape[-2], :batch['gt'].shape[-1]]\n",
    "            \n",
    "        res = metric_fn(pred, batch['gt'])\n",
    "        val_res.add(res.item(), inp.shape[0])\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description('val {:.4f}'.format(val_res.item()))\n",
    "            \n",
    "    return val_res.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(spec, tag=''):\n",
    "    if spec is None:\n",
    "        return None\n",
    "\n",
    "    dataset = datasets.make(spec['dataset'])\n",
    "    dataset = datasets.make(spec['wrapper'], args={'dataset': dataset})\n",
    "\n",
    "    log('{} dataset: size={}'.format(tag, len(dataset)))\n",
    "    for k, v in dataset[0].items():\n",
    "        log('  {}: shape={}'.format(k, tuple(v.shape)))\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n",
    "        shuffle=(tag == 'train'), num_workers=1, pin_memory=True)   # Changed number of workers since ram was not enough\n",
    "    return loader\n",
    "\n",
    "\n",
    "def make_data_loaders():\n",
    "    train_loader = make_data_loader(config.get('train_dataset'), tag='train')\n",
    "    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def prepare_training():\n",
    "#     if config.get('resume') is not None:\n",
    "    if os.path.exists(config.get('resume')):\n",
    "        sv_file = torch.load(config['resume'])\n",
    "        model = make(sv_file['model'], load_sd=True).cuda()\n",
    "        optimizer = make_optimizer(\n",
    "            model.parameters(), sv_file['optimizer'], load_sd=True)\n",
    "        epoch_start = sv_file['epoch'] + 1\n",
    "        if config.get('multi_step_lr') is None:\n",
    "            lr_scheduler = None\n",
    "        else:\n",
    "            lr_scheduler = MultiStepLR(optimizer, **config['multi_step_lr'])\n",
    "        for _ in range(epoch_start - 1):\n",
    "            lr_scheduler.step()\n",
    "    else:\n",
    "        model = make(config['model']).cuda()\n",
    "        optimizer = make_optimizer(\n",
    "            model.parameters(), config['optimizer'])\n",
    "        epoch_start = 1\n",
    "        if config.get('multi_step_lr') is None:\n",
    "            lr_scheduler = None\n",
    "        else:\n",
    "            lr_scheduler = MultiStepLR(optimizer, **config['multi_step_lr'])\n",
    "\n",
    "    log('model: #params={}'.format(compute_num_params(model, text=True)))\n",
    "    return model, optimizer, epoch_start, lr_scheduler\n",
    "\n",
    "\n",
    "def tmp_train(train_loader, model, optimizer, \\\n",
    "         epoch):\n",
    "    model.train()\n",
    "    loss_fn = nn.L1Loss()\n",
    "    train_loss = Averager()\n",
    "    metric_fn = calc_psnr\n",
    "\n",
    "    data_norm = config['data_norm']\n",
    "    t = data_norm['inp']\n",
    "    inp_sub = torch.FloatTensor(t['sub']).view(1, -1, 1, 1).cuda()\n",
    "    inp_div = torch.FloatTensor(t['div']).view(1, -1, 1, 1).cuda()\n",
    "    t = data_norm['gt']\n",
    "    gt_sub = torch.FloatTensor(t['sub']).view(1, 1, -1).cuda()\n",
    "    gt_div = torch.FloatTensor(t['div']).view(1, 1, -1).cuda()\n",
    "    \n",
    "    num_dataset = 800 # DIV2K\n",
    "    iter_per_epoch = int(num_dataset / config.get('train_dataset')['batch_size'] \\\n",
    "                        * config.get('train_dataset')['dataset']['args']['repeat'])\n",
    "    iteration = 0\n",
    "    for batch in tqdm(train_loader, leave=False, desc='train'):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.cuda()\n",
    "\n",
    "        inp = (batch['inp'] - inp_sub) / inp_div\n",
    "        pred = model(inp, batch['coord'], batch['cell'])\n",
    "\n",
    "        gt = (batch['gt'] - gt_sub) / gt_div\n",
    "        loss = loss_fn(pred, gt)\n",
    "        psnr = metric_fn(pred, gt)\n",
    "        \n",
    "        # tensorboard\n",
    "        writer.add_scalars('loss', {'train': loss.item()}, (epoch-1)*iter_per_epoch + iteration)\n",
    "        writer.add_scalars('psnr', {'train': psnr}, (epoch-1)*iter_per_epoch + iteration)\n",
    "        iteration += 1\n",
    "\n",
    "        train_loss.add(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = None; loss = None\n",
    "        \n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def main(config_, save_path):\n",
    "    global config, log, writer\n",
    "    config = config_\n",
    "    log, writer = set_save_path(save_path, remove=False)\n",
    "    with open(os.path.join(save_path, 'config.yaml'), 'w') as f:\n",
    "        yaml.dump(config, f, sort_keys=False)\n",
    "\n",
    "    train_loader, val_loader = make_data_loaders()\n",
    "    if config.get('data_norm') is None:\n",
    "        config['data_norm'] = {\n",
    "            'inp': {'sub': [0], 'div': [1]},\n",
    "            'gt': {'sub': [0], 'div': [1]}\n",
    "        }\n",
    "\n",
    "    model, optimizer, epoch_start, lr_scheduler = prepare_training()\n",
    "\n",
    "    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n",
    "    if n_gpus > 1:\n",
    "        model = nn.parallel.DataParallel(model)\n",
    "\n",
    "    epoch_max = config['epoch_max']\n",
    "    epoch_val = config.get('epoch_val')\n",
    "    epoch_save = config.get('epoch_save')\n",
    "    max_val_v = -1e18\n",
    "\n",
    "    timer = Timer()\n",
    "\n",
    "    for epoch in range(epoch_start, epoch_max + 1):\n",
    "        t_epoch_start = timer.t()\n",
    "        log_info = ['epoch {}/{}'.format(epoch, epoch_max)]\n",
    "\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        train_loss = tmp_train(train_loader, model, optimizer, \\\n",
    "                           epoch)\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        log_info.append('train: loss={:.4f}'.format(train_loss))\n",
    "#         writer.add_scalars('loss', {'train': train_loss}, epoch)\n",
    "\n",
    "        if n_gpus > 1:\n",
    "            model_ = model.module\n",
    "        else:\n",
    "            model_ = model\n",
    "        model_spec = config['model']\n",
    "        model_spec['sd'] = model_.state_dict()\n",
    "        optimizer_spec = config['optimizer']\n",
    "        optimizer_spec['sd'] = optimizer.state_dict()\n",
    "        sv_file = {\n",
    "            'model': model_spec,\n",
    "            'optimizer': optimizer_spec,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "\n",
    "        torch.save(sv_file, os.path.join(save_path, 'epoch-last.pth'))\n",
    "\n",
    "        if (epoch_save is not None) and (epoch % epoch_save == 0):\n",
    "            torch.save(sv_file,\n",
    "                os.path.join(save_path, 'epoch-{}.pth'.format(epoch)))\n",
    "\n",
    "        if (epoch_val is not None) and (epoch % epoch_val == 0):\n",
    "            if n_gpus > 1 and (config.get('eval_bsize') is not None):\n",
    "                model_ = model.module\n",
    "            else:\n",
    "                model_ = model\n",
    "            val_res = eval_psnr(val_loader, model_,\n",
    "                data_norm=config['data_norm'],\n",
    "                eval_type=config.get('eval_type'),\n",
    "                eval_bsize=config.get('eval_bsize'))\n",
    "\n",
    "            log_info.append('val: psnr={:.4f}'.format(val_res))\n",
    "#             writer.add_scalars('psnr', {'val': val_res}, epoch)\n",
    "            if val_res > max_val_v:\n",
    "                max_val_v = val_res\n",
    "                torch.save(sv_file, os.path.join(save_path, 'epoch-best.pth'))\n",
    "\n",
    "        t = timer.t()\n",
    "        prog = (epoch - epoch_start + 1) / (epoch_max - epoch_start + 1)\n",
    "        t_epoch = time_text(t - t_epoch_start)\n",
    "        t_elapsed, t_all = time_text(t), time_text(t / prog)\n",
    "        log_info.append('{} {}/{}'.format(t_epoch, t_elapsed, t_all))\n",
    "\n",
    "        log(', '.join(log_info))\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "def train(config_path, name=None, tag=None, gpu='0'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        print('config loaded.')\n",
    "\n",
    "    save_name = name\n",
    "    if save_name is None:\n",
    "        save_name = '_' + config_path.split('/')[-1][:-len('.yaml')]\n",
    "    if tag is not None:\n",
    "        save_name += '_' + tag\n",
    "    save_path = os.path.join('./save', save_name)\n",
    "    \n",
    "    main(config, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(config_path='configs/train.yaml', gpu='0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model, window=0, scale_max=4, fast=False, gpu='0'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "    with open(config, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    spec = config['test_dataset']\n",
    "    dataset = datasets.make(spec['dataset'])\n",
    "    dataset = datasets.make(spec['wrapper'], args={'dataset': dataset})\n",
    "    loader = DataLoader(dataset, batch_size=spec['batch_size'], num_workers=8, pin_memory=True)\n",
    "\n",
    "    model_spec = torch.load(model)['model']\n",
    "    model = make(model_spec, load_sd=True).cuda()\n",
    "\n",
    "    res = eval_psnr(loader, model,\n",
    "        data_norm=config.get('data_norm'),\n",
    "        eval_type=config.get('eval_type'),\n",
    "        eval_bsize=config.get('eval_bsize'),\n",
    "        window_size=int(window),\n",
    "        scale_max = int(scale_max),\n",
    "        fast = fast,\n",
    "        verbose=True)\n",
    "    print('result: {:.4f}'.format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Applications\\Anaconda\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 31.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test(config='configs/test.yaml', model='save/SwinIR_epoch-last.pth', window=8, gpu='0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss curve that progressed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def running_average(x, y, N):\n",
    "    y_ = np.convolve(y, np.ones(N)/N, mode='valid')\n",
    "    x_ = x[N//2:-N//2+1]\n",
    "    return x_, y_\n",
    "\n",
    "\n",
    "log_directories = [\n",
    "                        # '1e-4_4x6_300_LTE-SwinIR',\n",
    "                        # '2e-4_2x6_600_LTE-SwinIR', \n",
    "                        # '2e-4_2x6_900_LTE-SwinIR', \n",
    "                        # '2e-4_6x2_900_LTE-SwinIR',\n",
    "                        # '2e-4_6x6_300_LTE-SwinIR',\n",
    "                        # '5e-5_4x6_300_LTE-SwinIR', \n",
    "                        '2e-4_3x6_600_LTE-HAT_Final',\n",
    "                        '2e-4_3x6_600_LTE-SwinIR_Final'\n",
    "                  ]\n",
    "\n",
    "all_losses = []\n",
    "for log in log_directories:\n",
    "    descriptor = log                                    # The name of the folder is the descriptor\n",
    "    log_file = open(os.path.join('Training Trials', log, 'log.txt'), \"r\")              # Open log.txt\n",
    "    log = log_file.readlines()\n",
    "    losses_over_epochs = []\n",
    "    epochs = []                                         \n",
    "    psnrs = []\n",
    "    epoch_counter = 1\n",
    "    for line in log:                                    # Iterate over the lines in log.txt\n",
    "        if line[0] == 'e':                              # Lines that start with e are the appropriate ones\n",
    "            loss = float(line.split()[3][5:11])         # When spit, 3 is loss=######, 5:11 gives the ####, and then we convert to float\n",
    "            losses_over_epochs.append(loss)\n",
    "            epochs.append(epoch_counter)\n",
    "            epoch_counter += 1\n",
    "\n",
    "    log_file.close()                                    # Close the file when done reading it\n",
    "    all_losses.append((descriptor, epochs, losses_over_epochs))\n",
    "\n",
    "# Loss plot\n",
    "N = 50\n",
    "for descriptor, epochs, losses_over_epochs in all_losses:\n",
    "    # plt.plot(epochs, losses_over_epochs, label=descriptor)    # Raw data\n",
    "    epoch_average, loss_average = running_average(epochs, losses_over_epochs, N)\n",
    "    plt.plot(epoch_average, loss_average, label=str(descriptor))\n",
    "plt.title('Training loss over epochs (Running Average of %d)' % N)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce7721647274386fe13b7f2220e2cccc8d7a3ea400a265b5456a0f9751e59dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
